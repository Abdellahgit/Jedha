{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading libraries and Original Dataset<br />\n",
    "Dataset Source: <a href=\"https://www.kaggle.com/datasets/arshkon/linkedin-job-postings/data\">LinkedIn Job Postings (2023 - 2024)</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing basic libraries\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "# importing local file which contains a dictionnary\n",
    "from state_dict import *\n",
    "\n",
    "# Dataset sources folder \n",
    "path_to_file = '../Datasets/Linkedin_Job_Posting/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up stuff for pandas reading\n",
    "# Displaying all columns\n",
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Jobposting CSV\n",
    "dataset = pd.read_csv(path_to_file+'postings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dataset columns names\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the list of States in the location field and adding it to the dataset\n",
    "all_locations = dataset['location'].tolist()\n",
    "# Adding a new column in our dataset to store the State\n",
    "dataset['state'] = str(0)\n",
    "\n",
    "# Temp var\n",
    "temp_states = []\n",
    "\n",
    "# We need to split the data to only keep the States\n",
    "# Some entries are in the form \"city, State\", some are just generic text of city name (or \"United State\")\n",
    "for i in all_locations:\n",
    "    split_loc = [x.strip() for x in i.split(\",\")]\n",
    "    if len(split_loc) > 1:\n",
    "        # We have a lot of State with a loc in the form of \"STATE Metropolitan Area\" so we remove those parts\n",
    "        state = split_loc[1].replace(' Area', '')\n",
    "        state = state.replace(' Metropolitan', '')\n",
    "\n",
    "        # If the result is 'United States' or a two-digits letter name, we keep it as is\n",
    "        if (state == 'United States') or (len(state) == 2):\n",
    "            temp_states.append(state)\n",
    "        # Otherwise, we convert the full name to a two-digits letter name\n",
    "        elif state in state_conversion:\n",
    "            temp_states.append(state_conversion[state])\n",
    "        # Almost no occurences - we store \"Other\"\n",
    "        else:\n",
    "            temp_states.append('Other')\n",
    "    else:\n",
    "        # We keep the \"United States\" value but override the rest with Other as it's about 4% of the full dataset\n",
    "        if i.strip() == 'United States':\n",
    "            temp_states.append('United States')\n",
    "        else:\n",
    "            temp_states.append('Other')\n",
    "\n",
    "# temp_states\n",
    "\n",
    "# Storing the values in our dataset\n",
    "# TODO : optimize this as it takes about 10 seconds to just copy data\n",
    "for i in range(len(temp_states)):\n",
    "    dataset.loc[i, 'state'] = temp_states[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First File is About all \"Data\" Offers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter all the data to only keep job titles containing the word \"Data\" \n",
    "mask = dataset['title'].str.contains('data', case=False)\n",
    "subdf_jobs = dataset[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop useless columns\n",
    "columns_to_drop = ['views', 'formatted_work_type', 'applies', 'original_listed_time', 'job_posting_url', 'application_url', 'application_type', 'expiry', 'closed_time', 'skills_desc', 'listed_time', 'posting_domain', 'sponsored', 'compensation_type', 'max_salary', 'pay_period', 'med_salary', 'min_salary', 'currency', 'description']\n",
    "subdf_jobs_cleaned = subdf_jobs.drop(labels = columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>company_id</th>\n",
       "      <th>remote_allowed</th>\n",
       "      <th>formatted_experience_level</th>\n",
       "      <th>work_type</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>3245063922</td>\n",
       "      <td>Saxon AI</td>\n",
       "      <td>Data Architect</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>224935.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CONTRACT</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>3398076960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Technical Product and IT Manager for Data Cent...</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>3540371917</td>\n",
       "      <td>KeyBank</td>\n",
       "      <td>Enterprise Data &amp; Analytics Infrastructure Man...</td>\n",
       "      <td>Cleveland, OH</td>\n",
       "      <td>3252.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>3742692445</td>\n",
       "      <td>ZenithMinds Inc</td>\n",
       "      <td>Sr Data Engineer with Kafka</td>\n",
       "      <td>Austin, TX</td>\n",
       "      <td>81941852.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>3792233622</td>\n",
       "      <td>PB Built</td>\n",
       "      <td>Receptionist/Data Entry</td>\n",
       "      <td>Jupiter, FL</td>\n",
       "      <td>2331524.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         job_id     company_name  \\\n",
       "116  3245063922         Saxon AI   \n",
       "134  3398076960              NaN   \n",
       "165  3540371917          KeyBank   \n",
       "283  3742692445  ZenithMinds Inc   \n",
       "348  3792233622         PB Built   \n",
       "\n",
       "                                                 title           location  \\\n",
       "116                                     Data Architect  San Francisco, CA   \n",
       "134  Technical Product and IT Manager for Data Cent...      United States   \n",
       "165  Enterprise Data & Analytics Infrastructure Man...      Cleveland, OH   \n",
       "283                        Sr Data Engineer with Kafka         Austin, TX   \n",
       "348                            Receptionist/Data Entry        Jupiter, FL   \n",
       "\n",
       "     company_id  remote_allowed formatted_experience_level  work_type  \\\n",
       "116    224935.0             NaN                        NaN   CONTRACT   \n",
       "134         NaN             1.0                        NaN  FULL_TIME   \n",
       "165      3252.0             NaN                        NaN  FULL_TIME   \n",
       "283  81941852.0             1.0                        NaN  FULL_TIME   \n",
       "348   2331524.0             NaN                        NaN  FULL_TIME   \n",
       "\n",
       "             state  \n",
       "116             CA  \n",
       "134  United States  \n",
       "165             OH  \n",
       "283             TX  \n",
       "348             FL  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdf_jobs_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We write the sub df content into a new CSV file\n",
    "filename = 'all_offers_data.csv'\n",
    "\n",
    "#deleting file if a version already exists\n",
    "if filename in os.listdir(\"./csv\"):\n",
    "    os.remove('./csv/'+filename)\n",
    "\n",
    "# Writing the CSV\n",
    "subdf_jobs_cleaned.to_csv('./csv/'+filename, sep=',', na_rep='N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second One is about the ratio of Remote options per States\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new Dataset where we will save our data\n",
    "# Data will be stats per Location (States, mostly) about the share of remote work\n",
    "# We will need the State, but also data on number of offer in that state, number with remote option, and a share of remote (which is remote/total)\n",
    "\n",
    "subdf_remote = pd.DataFrame()\n",
    "subdf_remote['state'] = str(0)\n",
    "subdf_remote['total_offer'] = 0\n",
    "subdf_remote['remote_offer'] = 0\n",
    "subdf_remote['remote_share'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace all NaN values in the dataset['remote_allowed'] column by 0, as it works as a True/False and save it in a new dataset \n",
    "# df1 is an intermediate DF for cleaning data and stuff\n",
    "df1 = pd.DataFrame()\n",
    "df1['remote_allowed'] = dataset['remote_allowed']\n",
    "df1.fillna({'remote_allowed':0}, inplace=True)\n",
    "df1['state'] = dataset['state']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look for the total number of offers per states and populate our main DataFrame\n",
    "subset = df1.groupby('state', as_index=False).count()\n",
    "subdf_remote['state'] = subset['state']\n",
    "subdf_remote['total_offer'] = subset['remote_allowed']\n",
    "\n",
    "#Now we make a sum to only get the number of remote offers per state\n",
    "subset = df1.groupby('state', as_index=False).sum()\n",
    "subdf_remote['remote_offer'] = subset['remote_allowed'].astype('int64')\n",
    "\n",
    "# Finally, we calculate the share of remote work for each State\n",
    "for i in range(len(subdf_remote)):\n",
    "    subdf_remote.loc[i, 'remote_share'] = round(subdf_remote.loc[i, 'remote_offer']/subdf_remote.loc[i, 'total_offer']*100, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make another csv with this data\n",
    "filename = 'remote_work_share.csv'\n",
    "\n",
    "#deleting file if a version already exists\n",
    "if filename in os.listdir(\"./csv\"):\n",
    "    os.remove('./csv/'+filename)\n",
    "\n",
    "# Writing the CSV\n",
    "subdf_remote.to_csv('./csv/'+filename, sep=',', na_rep='N/A')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
