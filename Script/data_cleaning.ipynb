{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading libraries and Original Dataset<br />\n",
    "Dataset Source: <a href=\"https://www.kaggle.com/datasets/arshkon/linkedin-job-postings/data\">LinkedIn Job Postings (2023 - 2024)</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing basic libraries\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "# importing local file which contains a dictionnary\n",
    "from state_dict import *\n",
    "\n",
    "# Dataset sources folder \n",
    "path_to_file = '../Datasets/Linkedin_Job_Posting/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up stuff for pandas reading\n",
    "# Displaying all columns\n",
    "pd.set_option('display.max_columns', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Jobposting CSV\n",
    "dataset = pd.read_csv(path_to_file+'postings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dataset columns names\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the list of States in the location field and adding it to the dataset\n",
    "all_locations = dataset['location'].tolist()\n",
    "# Adding a new column in our dataset to store the State\n",
    "dataset['state'] = str(0)\n",
    "\n",
    "# Temp var\n",
    "temp_states = []\n",
    "\n",
    "# We need to split the data to only keep the States\n",
    "# Some entries are in the form \"city, State\", some are just generic text of city name (or \"United State\")\n",
    "for i in all_locations:\n",
    "    split_loc = [x.strip() for x in i.split(\",\")]\n",
    "    if len(split_loc) > 1:\n",
    "        # We have a lot of State with a loc in the form of \"STATE Metropolitan Area\" so we remove those parts\n",
    "        state = split_loc[1].replace(' Area', '')\n",
    "        state = state.replace(' Metropolitan', '')\n",
    "\n",
    "        # If the result is 'United States', we check if the first element is in our State Dictionnary, OTherwise we store \"United States\"\n",
    "        if state == 'United States':\n",
    "            if split_loc[0] in state_conversion:\n",
    "                temp_states.append(state_conversion[split_loc[0]])\n",
    "            else:\n",
    "                temp_states.append(state)\n",
    "        # We check if we already have a two-digits letter name, we keep it as is\n",
    "        elif len(state) == 2:\n",
    "            temp_states.append(state)\n",
    "        # Otherwise, we convert the full name to a two-digits letter name\n",
    "        elif state in state_conversion:\n",
    "            temp_states.append(state_conversion[state])\n",
    "        # Almost no occurences - we store \"Other\"\n",
    "        else:\n",
    "            temp_states.append('Other')\n",
    "    else:\n",
    "        # We keep the \"United States\" value but override the rest with Other as it's about 4% of the full dataset\n",
    "        if i.strip() == 'United States':\n",
    "            temp_states.append('United States')\n",
    "        else:\n",
    "            temp_states.append('Other')\n",
    "\n",
    "# temp_states\n",
    "\n",
    "# Storing the values in our dataset\n",
    "# TODO : optimize this as it takes about 10 seconds to just copy data\n",
    "for i in range(len(temp_states)):\n",
    "    dataset.loc[i, 'state'] = temp_states[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First File is About all \"Data\" Offers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter all the data to only keep job titles containing the word \"Data\" \n",
    "mask = dataset['title'].str.contains('data', case=False)\n",
    "subdf_jobs = dataset[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop useless columns\n",
    "\n",
    "# We have two entries, one in comments, because I modified the original CSV to remove a column from there\n",
    "# The column was responsible for over 450Mo of data and prevented us to easily upload the file to github\n",
    "\n",
    "# columns_to_drop = ['views', 'formatted_work_type', 'applies', 'original_listed_time', 'job_posting_url', 'application_url', 'application_type', 'expiry', 'closed_time', 'skills_desc', 'listed_time', 'posting_domain', 'sponsored', 'compensation_type', 'max_salary', 'pay_period', 'med_salary', 'min_salary', 'currency', 'description']\n",
    "columns_to_drop = ['views', 'formatted_work_type', 'applies', 'original_listed_time', 'job_posting_url', 'application_url', 'application_type', 'expiry', 'closed_time', 'skills_desc', 'listed_time', 'posting_domain', 'sponsored', 'compensation_type', 'max_salary', 'pay_period', 'med_salary', 'min_salary', 'currency']\n",
    "subdf_jobs_cleaned = subdf_jobs.drop(labels = columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We write the sub df content into a new CSV file\n",
    "filename = 'all_offers_data.csv'\n",
    "\n",
    "#deleting file if a version already exists\n",
    "if filename in os.listdir(\"./csv\"):\n",
    "    os.remove('./csv/'+filename)\n",
    "\n",
    "# Writing the CSV\n",
    "subdf_jobs_cleaned.to_csv('./csv/'+filename, sep=',', na_rep='N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second One is about the ratio of Remote options per States\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new Dataset where we will save our data\n",
    "# Data will be stats per Location (States, mostly) about the share of remote work\n",
    "# We will need the State, but also data on number of offer in that state, number with remote option, and a share of remote (which is remote/total)\n",
    "# We also calculate Quartiles so we can split the data into groups for the Data Viz part\n",
    "\n",
    "subdf_remote = pd.DataFrame()\n",
    "subdf_remote['state'] = str(0)\n",
    "subdf_remote['total_offer'] = 0\n",
    "subdf_remote['remote_offer'] = 0\n",
    "subdf_remote['remote_share'] = 0\n",
    "subdf_remote['remote_quartile'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace all NaN values in the dataset['remote_allowed'] column by 0, as it works as a True/False and save it in a new dataset \n",
    "# df1 is an intermediate DF for cleaning data and stuff\n",
    "df1 = pd.DataFrame()\n",
    "df1['remote_allowed'] = dataset['remote_allowed']\n",
    "df1.fillna({'remote_allowed':0}, inplace=True)\n",
    "df1['state'] = dataset['state']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 2, 3, 4, 4, 4, 4, 1, 3, 3, 1, 2, 3, 4, 2, 2, 1, 1, 4, 3, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 4, 1, 2, 3, 3, 2, 1, 3, 4, 3, 1, 4, 1, 2, 3, 3, 3, 4, 4, 4, 4, 3, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# We look for the total number of offers per states and populate our main DataFrame\n",
    "subset = df1.groupby('state', as_index=False).count()\n",
    "subdf_remote['state'] = subset['state']\n",
    "subdf_remote['total_offer'] = subset['remote_allowed']\n",
    "\n",
    "#Now we make a sum to only get the number of remote offers per state\n",
    "subset = df1.groupby('state', as_index=False).sum()\n",
    "subdf_remote['remote_offer'] = subset['remote_allowed'].astype('int64')\n",
    "\n",
    "# We calculate the share of remote work for each State\n",
    "for i in range(len(subdf_remote)):\n",
    "    subdf_remote.loc[i, 'remote_share'] = round(subdf_remote.loc[i, 'remote_offer']/subdf_remote.loc[i, 'total_offer']*100, 2)\n",
    "\n",
    "# We calculate the quartile each State belongs to based on its share of remote work and set a score based on the quartile \n",
    "q1, q2, q3 = subdf_remote['remote_share'].quantile([0.25,0.5,0.75])\n",
    "\n",
    "temp_qtl = []\n",
    "\n",
    "for i in subdf_remote['remote_share']:\n",
    "    if i < q1:\n",
    "        temp_qtl.append(1)\n",
    "    elif i < q2:\n",
    "        temp_qtl.append(2)\n",
    "    elif i < q3:\n",
    "        temp_qtl.append(3)\n",
    "    else:\n",
    "        temp_qtl.append(4)\n",
    "\n",
    "for i in range(len(temp_qtl)):\n",
    "    subdf_remote.loc[i, 'remote_quartile'] = temp_qtl[i]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make another csv with this data\n",
    "filename = 'remote_work_share.csv'\n",
    "\n",
    "#deleting file if a version already exists\n",
    "if filename in os.listdir(\"./csv\"):\n",
    "    os.remove('./csv/'+filename)\n",
    "\n",
    "# Writing the CSV\n",
    "subdf_remote.to_csv('./csv/'+filename, sep=',', na_rep='N/A')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
